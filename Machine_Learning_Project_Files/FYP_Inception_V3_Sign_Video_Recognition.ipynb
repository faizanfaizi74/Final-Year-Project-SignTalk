{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07ZAtJ4wUOEI"
      },
      "source": [
        "## Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEWGQmw-gmmx"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import random\n",
        "import ssl\n",
        "import cv2\n",
        "import numpy as np\n",
        "import imageio\n",
        "from IPython import display\n",
        "from urllib import request\n",
        "import re\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "from keras import backend as K\n",
        "import sys\n",
        "import csv\n",
        "import os\n",
        "import math\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Sequential\n",
        "from keras.layers import Dense, InputLayer, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalMaxPooling2D\n",
        "\n",
        "from keras.layers import LSTM\n",
        "from keras.layers.convolutional import Conv1D\n",
        "from keras.layers.convolutional import MaxPooling1D\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50, InceptionResNetV2\n",
        "from google.colab import drive\n",
        "import gc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QNtosDMaj35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0dd79b0-32f1-4772-f458-71a4a4055632"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for tensorflow-docs (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q git+https://github.com/tensorflow/docs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abU8MJxzgxCv",
        "outputId": "040b24c6-5778-4a4c-a98a-4df0d36d2bc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "#importing google drive in google colab\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwQcmVJOft0I"
      },
      "source": [
        "## Define root paths accordingly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23zVEHeTxcK_"
      },
      "outputs": [],
      "source": [
        "data_root = '/content/gdrive/MyDrive/sign_videos_sample_a_6'\n",
        "folder_root = '/content/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_d-IHs8QURbf"
      },
      "source": [
        "## Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l6qQaJux4Ld",
        "outputId": "fd7fd008-8cdb-4f19-b1dd-ef8c76b0ebc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9W2jiz2lZpml"
      },
      "source": [
        "# CSV creation\n",
        "\n",
        "* Mention required video classnames\n",
        "* CSV columns structure\n",
        "  * Video path -> Class index (from 0 to 4 here, as we have five classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "J_UokD3Yra_A",
        "outputId": "a81809bc-feda-4b35-ea16-4fb64db1d5ff"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1440x1440 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#creating the Matplotlib figure and specify the size of the figure\n",
        "plt.figure(figsize=(20,20))\n",
        "\n",
        "classes = ['call_the_ambulance','i_am_a_student','i_can_not_speak', 'how_are_you', \"i_don't_understand\", 'extra_class'] # 5 a, 5 b\n",
        "\n",
        "\n",
        "with open(folder_root+'dataset.csv', 'w', newline='') as file:\n",
        "  writer = csv.writer(file)\n",
        "  for c in classes:\n",
        "    path = os.path.join(data_root, c+\"/\")\n",
        "    for i in os.listdir(path):\n",
        "      writer.writerow([classes.index(c), os.path.join(path, i)])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3fTo8eDZwZD"
      },
      "source": [
        "## Shuffling csv data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrokWb-jtvDD"
      },
      "outputs": [],
      "source": [
        "# shuffle the data in csv\n",
        "df = pd.read_csv(folder_root+'dataset.csv')\n",
        "ds = df.sample(frac=1)\n",
        "ds.to_csv(folder_root+'dataset.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bsnoc1dGuJ2U"
      },
      "outputs": [],
      "source": [
        "# split the data into train and test\n",
        "\n",
        "import numpy as np\n",
        "df = pd.read_csv(folder_root+'dataset.csv', header=None)\n",
        "df.columns = [\"class\", \"path\"]\n",
        "df = df.astype({\"class\": str})\n",
        "train, test = np.split(df.sample(frac=1, random_state=42), [int(.8*len(df))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqv2Gwq_l55E"
      },
      "source": [
        "## Defining hyperparameters\n",
        "* batch size\n",
        "* epochs\n",
        "* max sequence length\n",
        "* number of features\n",
        "* image size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBWyo_tAupsp"
      },
      "outputs": [],
      "source": [
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 256\n",
        "EPOCHS = 100\n",
        "MAX_SEQ_LENGTH = 100\n",
        "NUM_FEATURES = 2048"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ICiS4LsmK6R"
      },
      "source": [
        "Count of training and test videos :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "id": "4giyPQcOurLb",
        "outputId": "b21706c9-c505-43e3-ae10-11f60fc0a0e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total videos for training: 371\n",
            "Total videos for testing: 93\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    class                                               path\n",
              "79      4  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "362     4  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "162     0  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "68      5  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "194     5  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "227     5  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "157     4  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "107     2  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "0       0  /content/gdrive/MyDrive/sign_videos_sample_a_6...\n",
              "101     4  /content/gdrive/MyDrive/sign_videos_sample_a_6..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d8ec1eea-b6ed-4d8a-a387-62489108ec6a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>path</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>79</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>362</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>162</th>\n",
              "      <td>0</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>5</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>194</th>\n",
              "      <td>5</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>227</th>\n",
              "      <td>5</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>157</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>107</th>\n",
              "      <td>2</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>101</th>\n",
              "      <td>4</td>\n",
              "      <td>/content/gdrive/MyDrive/sign_videos_sample_a_6...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d8ec1eea-b6ed-4d8a-a387-62489108ec6a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d8ec1eea-b6ed-4d8a-a387-62489108ec6a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d8ec1eea-b6ed-4d8a-a387-62489108ec6a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "train_df = train\n",
        "test_df = test\n",
        "\n",
        "print(f\"Total videos for training: {len(train_df)}\")\n",
        "print(f\"Total videos for testing: {len(test_df)}\")\n",
        "\n",
        "train_df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7hvHVkEafMa"
      },
      "source": [
        "# UTIL functions from HB hub video classification tutorials\n",
        "\n",
        "TF Hub has several resources for video classification which have been extremely helpful. Some of the utilty functions have been used here.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "is1hWyBfvOi5"
      },
      "outputs": [],
      "source": [
        "def crop_center_square(frame):\n",
        "    y, x = frame.shape[0:2]\n",
        "    min_dim = min(y, x)\n",
        "    start_x = (x // 2) - (min_dim // 2)\n",
        "    start_y = (y // 2) - (min_dim // 2)\n",
        "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
        "    \n",
        "def load_video(path, max_frames=0, resize=(IMG_SIZE, IMG_SIZE)):\n",
        "  \n",
        "    cap = cv2.VideoCapture(path)\n",
        "    cap.set(cv2.CAP_PROP_POS_MSEC, 500)\n",
        "    frames = []\n",
        "    j = 0\n",
        "    try:\n",
        "        while True:\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "            #frame = crop_center_square(frame)\n",
        "            frame = cv2.resize(frame, resize)\n",
        "            frame = frame[:, :, [2, 1, 0]]    #BRG for cv2\n",
        "            frames.append(frame)\n",
        "            #cv2.imwrite(\"/content/frameimg\"+str(j)+\".jpg\", frame)\n",
        "\n",
        "            if len(frames) == max_frames:\n",
        "                break\n",
        "    finally:\n",
        "        cap.release()\n",
        "    return np.array(frames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpqvTPE4vgA_",
        "outputId": "8c2733f7-5ce0-4df9-b430-f9d139c9fbb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 1s 0us/step\n",
            "87924736/87910968 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "def build_feature_extractor():\n",
        "    feature_extractor = keras.applications.InceptionV3(\n",
        "        weights=\"imagenet\",\n",
        "        include_top=False,\n",
        "        pooling=\"avg\",\n",
        "        input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
        "    )\n",
        "    preprocess_input = keras.applications.inception_v3.preprocess_input\n",
        "\n",
        "    inputs = keras.Input((IMG_SIZE, IMG_SIZE, 3))\n",
        "    preprocessed = preprocess_input(inputs)\n",
        "\n",
        "    outputs = feature_extractor(preprocessed)\n",
        "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
        "\n",
        "\n",
        "feature_extractor = build_feature_extractor()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQPJfufovk8r",
        "outputId": "f12fa8e3-1025-4bc9-a688-3b9173749ec7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0', '1', '2', '3', '4', '5']\n"
          ]
        }
      ],
      "source": [
        "label_processor = keras.layers.experimental.preprocessing.StringLookup(\n",
        "    num_oov_indices=0, vocabulary=np.unique(train_df[\"class\"])\n",
        ")\n",
        "print(label_processor.get_vocabulary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcgMOGgWuvjr"
      },
      "outputs": [],
      "source": [
        "class_vocab = label_processor.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qA7F6qV4QcI",
        "outputId": "81bfcfb8-8d64-4c34-a61d-3fb020e3606e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12786"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVYdjJ5ZkjE_"
      },
      "source": [
        "# Dataset Preprocessing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwHNNowsv3gT"
      },
      "outputs": [],
      "source": [
        "def prepare_all_videos(df):\n",
        "    num_samples = len(df)\n",
        "    video_paths = df[\"path\"].values.tolist()\n",
        "    labels = df[\"class\"].values\n",
        "    labels = label_processor(labels[..., None]).numpy()\n",
        "\n",
        "    # `frame_masks` and `frame_features` are what we will feed to our sequence model.\n",
        "    # `frame_masks` will contain a bunch of booleans denoting if a timestep is\n",
        "    # masked with padding or not.\n",
        "    frame_masks = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH), dtype=\"bool\")\n",
        "    frame_features = np.zeros(\n",
        "        shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "    )\n",
        "\n",
        "    # For each video.\n",
        "    for idx,path in enumerate(video_paths):\n",
        "        # Gather all its frames and add a batch dimension.hike intern\n",
        "        #path = video_paths[idx]\n",
        "        frames = load_video(path)\n",
        "        frames = frames[None, ...]\n",
        "\n",
        "        # final_frames = np.append(train_frames, frames) # farmes has 5 dimensions error\n",
        "\n",
        "        gc.collect()\n",
        "        # Initialize placeholders to store the masks and features of the current video.\n",
        "        temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "        temp_frame_featutes = np.zeros(\n",
        "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
        "        )\n",
        "\n",
        "        # Extract features from the frames of the current video.\n",
        "        for i, batch in enumerate(frames):\n",
        "            try:\n",
        "              video_length = batch.shape[1]\n",
        "              length = min(MAX_SEQ_LENGTH, video_length)\n",
        "              for j in range(length):\n",
        "                temp_frame_featutes[i, j, :] = feature_extractor.predict(\n",
        "                    batch[None, j, :]\n",
        "                )\n",
        "              temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "              frame_features[idx,] = temp_frame_featutes.squeeze()\n",
        "              frame_masks[idx,] = temp_frame_mask.squeeze()\n",
        "            except:\n",
        "              pass\n",
        "\n",
        "        gc.collect()\n",
        "        print(idx)\n",
        "\n",
        "    return (frame_features, frame_masks), labels\n",
        "\n",
        "gc.collect()\n",
        "\n",
        "train_data, train_labels = prepare_all_videos(train_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KeSKeqt7jtU",
        "outputId": "f6c0a541-4ac3-40f4-a2cc-3353b0090f44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (371, 100, 2048)\n",
            "Frame masks in train set: (371, 100)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Frame features in train set: {train_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {train_data[1].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWVF0VUc67DJ"
      },
      "outputs": [],
      "source": [
        "# print(train_frames.shape)\n",
        "# print(len(train_frames))  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0nqrSos9w56"
      },
      "outputs": [],
      "source": [
        "# print(\"Lables Shape: \", train_labels.shape)\n",
        "# print(\"Frames Shape: \", train_frames.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8_-r7icJeLb",
        "outputId": "3392a63e-f4d2-4a8a-fd41-1add7400d748"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWj6tZYaZ-EE"
      },
      "source": [
        "Preparing test data :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jTXvkh67xqI"
      },
      "outputs": [],
      "source": [
        "test_data, test_labels = prepare_all_videos(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcLnt5X97oQj",
        "outputId": "c73ff92f-886f-4532-f458-cee6cfd8f9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame features in train set: (93, 100, 2048)\n",
            "Frame masks in train set: (93, 100)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Frame features in train set: {test_data[0].shape}\")\n",
        "print(f\"Frame masks in train set: {test_data[1].shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQOQPio8wDJw"
      },
      "outputs": [],
      "source": [
        "# Utility for our sequence model.\n",
        "def get_sequence_model():\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
        "    mask_input = keras.Input((MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "\n",
        "    # Refer to the following tutorial to understand the significance of using `mask`:\n",
        "    # https://keras.io/api/layers/recurrent_layers/gru/\n",
        "    x = keras.layers.GRU(16, return_sequences=True, go_backwards=True)(\n",
        "        frame_features_input, mask=mask_input\n",
        "    )\n",
        "    x = keras.layers.GRU(8)(x)\n",
        "    x = keras.layers.Dropout(0.25)(x)\n",
        "\n",
        "    x = keras.layers.Dense(8, activation=\"relu\")(x)\n",
        "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
        "\n",
        "    rnn_model = keras.Model([frame_features_input, mask_input], output)\n",
        "    \n",
        "    opt = keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "    rnn_model.compile(\n",
        "        loss=\"sparse_categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"]\n",
        "    )\n",
        "    return rnn_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdkTOyrKbZOo"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XD7aduXJwUUq",
        "outputId": "4c5e9d74-d9cb-4fe6-c42c-9395cfa5bc72"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.7728 - accuracy: 0.2086\n",
            "Epoch 1: val_loss improved from inf to 1.71029, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 17s 949ms/step - loss: 1.7728 - accuracy: 0.2086 - val_loss: 1.7103 - val_accuracy: 0.3978\n",
            "Epoch 2/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.6543 - accuracy: 0.4388\n",
            "Epoch 2: val_loss improved from 1.71029 to 1.61591, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 618ms/step - loss: 1.6543 - accuracy: 0.4388 - val_loss: 1.6159 - val_accuracy: 0.5161\n",
            "Epoch 3/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5860 - accuracy: 0.4928\n",
            "Epoch 3: val_loss improved from 1.61591 to 1.60061, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 613ms/step - loss: 1.5860 - accuracy: 0.4928 - val_loss: 1.6006 - val_accuracy: 0.5376\n",
            "Epoch 4/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5620 - accuracy: 0.5252\n",
            "Epoch 4: val_loss improved from 1.60061 to 1.55835, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 612ms/step - loss: 1.5620 - accuracy: 0.5252 - val_loss: 1.5583 - val_accuracy: 0.5054\n",
            "Epoch 5/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.5043 - accuracy: 0.5647\n",
            "Epoch 5: val_loss improved from 1.55835 to 1.54813, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 610ms/step - loss: 1.5043 - accuracy: 0.5647 - val_loss: 1.5481 - val_accuracy: 0.5376\n",
            "Epoch 6/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.4896 - accuracy: 0.5647\n",
            "Epoch 6: val_loss improved from 1.54813 to 1.53351, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 1.4896 - accuracy: 0.5647 - val_loss: 1.5335 - val_accuracy: 0.5376\n",
            "Epoch 7/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.4406 - accuracy: 0.5612\n",
            "Epoch 7: val_loss improved from 1.53351 to 1.50982, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 615ms/step - loss: 1.4406 - accuracy: 0.5612 - val_loss: 1.5098 - val_accuracy: 0.5376\n",
            "Epoch 8/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.4232 - accuracy: 0.5719\n",
            "Epoch 8: val_loss improved from 1.50982 to 1.48405, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 711ms/step - loss: 1.4232 - accuracy: 0.5719 - val_loss: 1.4841 - val_accuracy: 0.5484\n",
            "Epoch 9/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.4101 - accuracy: 0.5755\n",
            "Epoch 9: val_loss improved from 1.48405 to 1.47266, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 610ms/step - loss: 1.4101 - accuracy: 0.5755 - val_loss: 1.4727 - val_accuracy: 0.5269\n",
            "Epoch 10/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.3833 - accuracy: 0.5791\n",
            "Epoch 10: val_loss improved from 1.47266 to 1.45173, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 617ms/step - loss: 1.3833 - accuracy: 0.5791 - val_loss: 1.4517 - val_accuracy: 0.5591\n",
            "Epoch 11/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.3401 - accuracy: 0.5719\n",
            "Epoch 11: val_loss improved from 1.45173 to 1.43603, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 614ms/step - loss: 1.3401 - accuracy: 0.5719 - val_loss: 1.4360 - val_accuracy: 0.5591\n",
            "Epoch 12/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.3319 - accuracy: 0.6079\n",
            "Epoch 12: val_loss improved from 1.43603 to 1.39336, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 614ms/step - loss: 1.3319 - accuracy: 0.6079 - val_loss: 1.3934 - val_accuracy: 0.5484\n",
            "Epoch 13/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.2849 - accuracy: 0.6151\n",
            "Epoch 13: val_loss did not improve from 1.39336\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 1.2849 - accuracy: 0.6151 - val_loss: 1.4042 - val_accuracy: 0.6129\n",
            "Epoch 14/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.2624 - accuracy: 0.6151\n",
            "Epoch 14: val_loss improved from 1.39336 to 1.35768, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 1.2624 - accuracy: 0.6151 - val_loss: 1.3577 - val_accuracy: 0.5699\n",
            "Epoch 15/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.2292 - accuracy: 0.6331\n",
            "Epoch 15: val_loss did not improve from 1.35768\n",
            "9/9 [==============================] - 5s 604ms/step - loss: 1.2292 - accuracy: 0.6331 - val_loss: 1.3822 - val_accuracy: 0.5699\n",
            "Epoch 16/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.1849 - accuracy: 0.6691\n",
            "Epoch 16: val_loss improved from 1.35768 to 1.32005, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 612ms/step - loss: 1.1849 - accuracy: 0.6691 - val_loss: 1.3200 - val_accuracy: 0.6559\n",
            "Epoch 17/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.1752 - accuracy: 0.6547\n",
            "Epoch 17: val_loss improved from 1.32005 to 1.31041, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 612ms/step - loss: 1.1752 - accuracy: 0.6547 - val_loss: 1.3104 - val_accuracy: 0.6452\n",
            "Epoch 18/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.1584 - accuracy: 0.6511\n",
            "Epoch 18: val_loss improved from 1.31041 to 1.26209, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 609ms/step - loss: 1.1584 - accuracy: 0.6511 - val_loss: 1.2621 - val_accuracy: 0.6667\n",
            "Epoch 19/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.1234 - accuracy: 0.6978\n",
            "Epoch 19: val_loss improved from 1.26209 to 1.24740, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 623ms/step - loss: 1.1234 - accuracy: 0.6978 - val_loss: 1.2474 - val_accuracy: 0.6344\n",
            "Epoch 20/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0868 - accuracy: 0.6655\n",
            "Epoch 20: val_loss did not improve from 1.24740\n",
            "9/9 [==============================] - 5s 612ms/step - loss: 1.0868 - accuracy: 0.6655 - val_loss: 1.2702 - val_accuracy: 0.6344\n",
            "Epoch 21/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0585 - accuracy: 0.6942\n",
            "Epoch 21: val_loss improved from 1.24740 to 1.22408, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 626ms/step - loss: 1.0585 - accuracy: 0.6942 - val_loss: 1.2241 - val_accuracy: 0.6774\n",
            "Epoch 22/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 1.0263 - accuracy: 0.7230\n",
            "Epoch 22: val_loss improved from 1.22408 to 1.18812, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 1.0263 - accuracy: 0.7230 - val_loss: 1.1881 - val_accuracy: 0.6882\n",
            "Epoch 23/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9867 - accuracy: 0.7266\n",
            "Epoch 23: val_loss improved from 1.18812 to 1.16174, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 0.9867 - accuracy: 0.7266 - val_loss: 1.1617 - val_accuracy: 0.6452\n",
            "Epoch 24/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9635 - accuracy: 0.7194\n",
            "Epoch 24: val_loss did not improve from 1.16174\n",
            "9/9 [==============================] - 5s 607ms/step - loss: 0.9635 - accuracy: 0.7194 - val_loss: 1.2363 - val_accuracy: 0.6452\n",
            "Epoch 25/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9435 - accuracy: 0.7158\n",
            "Epoch 25: val_loss did not improve from 1.16174\n",
            "9/9 [==============================] - 5s 604ms/step - loss: 0.9435 - accuracy: 0.7158 - val_loss: 1.1664 - val_accuracy: 0.6882\n",
            "Epoch 26/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9459 - accuracy: 0.7302\n",
            "Epoch 26: val_loss improved from 1.16174 to 1.14669, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 605ms/step - loss: 0.9459 - accuracy: 0.7302 - val_loss: 1.1467 - val_accuracy: 0.6774\n",
            "Epoch 27/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9101 - accuracy: 0.7482\n",
            "Epoch 27: val_loss improved from 1.14669 to 1.11639, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.9101 - accuracy: 0.7482 - val_loss: 1.1164 - val_accuracy: 0.6989\n",
            "Epoch 28/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.9126 - accuracy: 0.7122\n",
            "Epoch 28: val_loss improved from 1.11639 to 1.08977, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 615ms/step - loss: 0.9126 - accuracy: 0.7122 - val_loss: 1.0898 - val_accuracy: 0.7312\n",
            "Epoch 29/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.8450 - accuracy: 0.7662\n",
            "Epoch 29: val_loss improved from 1.08977 to 1.06670, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.8450 - accuracy: 0.7662 - val_loss: 1.0667 - val_accuracy: 0.7419\n",
            "Epoch 30/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.8075 - accuracy: 0.8129\n",
            "Epoch 30: val_loss did not improve from 1.06670\n",
            "9/9 [==============================] - 5s 607ms/step - loss: 0.8075 - accuracy: 0.8129 - val_loss: 1.1277 - val_accuracy: 0.7204\n",
            "Epoch 31/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.7729 - accuracy: 0.8417\n",
            "Epoch 31: val_loss did not improve from 1.06670\n",
            "9/9 [==============================] - 5s 610ms/step - loss: 0.7729 - accuracy: 0.8417 - val_loss: 1.0685 - val_accuracy: 0.7634\n",
            "Epoch 32/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.7651 - accuracy: 0.8309\n",
            "Epoch 32: val_loss improved from 1.06670 to 1.04368, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 613ms/step - loss: 0.7651 - accuracy: 0.8309 - val_loss: 1.0437 - val_accuracy: 0.7849\n",
            "Epoch 33/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.7386 - accuracy: 0.8417\n",
            "Epoch 33: val_loss did not improve from 1.04368\n",
            "9/9 [==============================] - 5s 610ms/step - loss: 0.7386 - accuracy: 0.8417 - val_loss: 1.0719 - val_accuracy: 0.7742\n",
            "Epoch 34/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.7331 - accuracy: 0.8453\n",
            "Epoch 34: val_loss improved from 1.04368 to 1.01885, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 616ms/step - loss: 0.7331 - accuracy: 0.8453 - val_loss: 1.0188 - val_accuracy: 0.7419\n",
            "Epoch 35/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6776 - accuracy: 0.7842\n",
            "Epoch 35: val_loss did not improve from 1.01885\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 0.6776 - accuracy: 0.7842 - val_loss: 1.0420 - val_accuracy: 0.6237\n",
            "Epoch 36/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6679 - accuracy: 0.7410\n",
            "Epoch 36: val_loss improved from 1.01885 to 0.98689, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 612ms/step - loss: 0.6679 - accuracy: 0.7410 - val_loss: 0.9869 - val_accuracy: 0.6129\n",
            "Epoch 37/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6594 - accuracy: 0.7410\n",
            "Epoch 37: val_loss did not improve from 0.98689\n",
            "9/9 [==============================] - 5s 605ms/step - loss: 0.6594 - accuracy: 0.7410 - val_loss: 0.9926 - val_accuracy: 0.6237\n",
            "Epoch 38/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6514 - accuracy: 0.7302\n",
            "Epoch 38: val_loss improved from 0.98689 to 0.97242, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 616ms/step - loss: 0.6514 - accuracy: 0.7302 - val_loss: 0.9724 - val_accuracy: 0.5914\n",
            "Epoch 39/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6360 - accuracy: 0.7446\n",
            "Epoch 39: val_loss improved from 0.97242 to 0.92638, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 0.6360 - accuracy: 0.7446 - val_loss: 0.9264 - val_accuracy: 0.6129\n",
            "Epoch 40/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6101 - accuracy: 0.7554\n",
            "Epoch 40: val_loss did not improve from 0.92638\n",
            "9/9 [==============================] - 5s 609ms/step - loss: 0.6101 - accuracy: 0.7554 - val_loss: 0.9875 - val_accuracy: 0.6022\n",
            "Epoch 41/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5773 - accuracy: 0.7770\n",
            "Epoch 41: val_loss improved from 0.92638 to 0.89962, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 0.5773 - accuracy: 0.7770 - val_loss: 0.8996 - val_accuracy: 0.6129\n",
            "Epoch 42/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5756 - accuracy: 0.7626\n",
            "Epoch 42: val_loss improved from 0.89962 to 0.88738, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 616ms/step - loss: 0.5756 - accuracy: 0.7626 - val_loss: 0.8874 - val_accuracy: 0.6237\n",
            "Epoch 43/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5430 - accuracy: 0.7698\n",
            "Epoch 43: val_loss did not improve from 0.88738\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.5430 - accuracy: 0.7698 - val_loss: 0.8931 - val_accuracy: 0.6129\n",
            "Epoch 44/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.7770\n",
            "Epoch 44: val_loss did not improve from 0.88738\n",
            "9/9 [==============================] - 5s 605ms/step - loss: 0.5366 - accuracy: 0.7770 - val_loss: 0.9235 - val_accuracy: 0.6129\n",
            "Epoch 45/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.7770\n",
            "Epoch 45: val_loss did not improve from 0.88738\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 0.5248 - accuracy: 0.7770 - val_loss: 0.8970 - val_accuracy: 0.6344\n",
            "Epoch 46/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5261 - accuracy: 0.7770\n",
            "Epoch 46: val_loss improved from 0.88738 to 0.86617, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 612ms/step - loss: 0.5261 - accuracy: 0.7770 - val_loss: 0.8662 - val_accuracy: 0.6559\n",
            "Epoch 47/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5288 - accuracy: 0.7770\n",
            "Epoch 47: val_loss improved from 0.86617 to 0.85506, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 616ms/step - loss: 0.5288 - accuracy: 0.7770 - val_loss: 0.8551 - val_accuracy: 0.6344\n",
            "Epoch 48/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5143 - accuracy: 0.7698\n",
            "Epoch 48: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 6s 615ms/step - loss: 0.5143 - accuracy: 0.7698 - val_loss: 0.8582 - val_accuracy: 0.6344\n",
            "Epoch 49/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4836 - accuracy: 0.7770\n",
            "Epoch 49: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 0.4836 - accuracy: 0.7770 - val_loss: 0.8771 - val_accuracy: 0.6344\n",
            "Epoch 50/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4837 - accuracy: 0.8345\n",
            "Epoch 50: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 5s 607ms/step - loss: 0.4837 - accuracy: 0.8345 - val_loss: 0.8960 - val_accuracy: 0.7957\n",
            "Epoch 51/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4883 - accuracy: 0.9029\n",
            "Epoch 51: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.4883 - accuracy: 0.9029 - val_loss: 0.8978 - val_accuracy: 0.7742\n",
            "Epoch 52/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4815 - accuracy: 0.8993\n",
            "Epoch 52: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 5s 607ms/step - loss: 0.4815 - accuracy: 0.8993 - val_loss: 0.9396 - val_accuracy: 0.7849\n",
            "Epoch 53/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4814 - accuracy: 0.8957\n",
            "Epoch 53: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 5s 612ms/step - loss: 0.4814 - accuracy: 0.8957 - val_loss: 0.8756 - val_accuracy: 0.8172\n",
            "Epoch 54/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4664 - accuracy: 0.9029\n",
            "Epoch 54: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 6s 617ms/step - loss: 0.4664 - accuracy: 0.9029 - val_loss: 0.8983 - val_accuracy: 0.7957\n",
            "Epoch 55/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4719 - accuracy: 0.9029\n",
            "Epoch 55: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 6s 615ms/step - loss: 0.4719 - accuracy: 0.9029 - val_loss: 0.8774 - val_accuracy: 0.8172\n",
            "Epoch 56/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4826 - accuracy: 0.8921\n",
            "Epoch 56: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.4826 - accuracy: 0.8921 - val_loss: 0.9009 - val_accuracy: 0.8065\n",
            "Epoch 57/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4634 - accuracy: 0.8993\n",
            "Epoch 57: val_loss did not improve from 0.85506\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.4634 - accuracy: 0.8993 - val_loss: 0.9288 - val_accuracy: 0.7634\n",
            "Epoch 58/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4413 - accuracy: 0.9101\n",
            "Epoch 58: val_loss improved from 0.85506 to 0.84966, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 6s 615ms/step - loss: 0.4413 - accuracy: 0.9101 - val_loss: 0.8497 - val_accuracy: 0.8172\n",
            "Epoch 59/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4427 - accuracy: 0.9029\n",
            "Epoch 59: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 608ms/step - loss: 0.4427 - accuracy: 0.9029 - val_loss: 0.9194 - val_accuracy: 0.7849\n",
            "Epoch 60/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4641 - accuracy: 0.8993\n",
            "Epoch 60: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 613ms/step - loss: 0.4641 - accuracy: 0.8993 - val_loss: 0.9370 - val_accuracy: 0.7957\n",
            "Epoch 61/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4465 - accuracy: 0.8993\n",
            "Epoch 61: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 608ms/step - loss: 0.4465 - accuracy: 0.8993 - val_loss: 0.9812 - val_accuracy: 0.7742\n",
            "Epoch 62/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4400 - accuracy: 0.9029\n",
            "Epoch 62: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 604ms/step - loss: 0.4400 - accuracy: 0.9029 - val_loss: 0.9082 - val_accuracy: 0.7849\n",
            "Epoch 63/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4307 - accuracy: 0.9065\n",
            "Epoch 63: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 612ms/step - loss: 0.4307 - accuracy: 0.9065 - val_loss: 0.9440 - val_accuracy: 0.7957\n",
            "Epoch 64/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5066 - accuracy: 0.8849\n",
            "Epoch 64: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 602ms/step - loss: 0.5066 - accuracy: 0.8849 - val_loss: 1.0726 - val_accuracy: 0.7312\n",
            "Epoch 65/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4784 - accuracy: 0.8849\n",
            "Epoch 65: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.4784 - accuracy: 0.8849 - val_loss: 1.2047 - val_accuracy: 0.7742\n",
            "Epoch 66/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4835 - accuracy: 0.8849\n",
            "Epoch 66: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 607ms/step - loss: 0.4835 - accuracy: 0.8849 - val_loss: 1.6222 - val_accuracy: 0.6989\n",
            "Epoch 67/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6418 - accuracy: 0.8417\n",
            "Epoch 67: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.6418 - accuracy: 0.8417 - val_loss: 1.3029 - val_accuracy: 0.6237\n",
            "Epoch 68/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6709 - accuracy: 0.8201\n",
            "Epoch 68: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 609ms/step - loss: 0.6709 - accuracy: 0.8201 - val_loss: 1.1963 - val_accuracy: 0.7312\n",
            "Epoch 69/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.5638 - accuracy: 0.8561\n",
            "Epoch 69: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 610ms/step - loss: 0.5638 - accuracy: 0.8561 - val_loss: 1.1118 - val_accuracy: 0.7312\n",
            "Epoch 70/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.6258 - accuracy: 0.8381\n",
            "Epoch 70: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 609ms/step - loss: 0.6258 - accuracy: 0.8381 - val_loss: 1.2464 - val_accuracy: 0.7097\n",
            "Epoch 71/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4979 - accuracy: 0.8813\n",
            "Epoch 71: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 0.4979 - accuracy: 0.8813 - val_loss: 1.2081 - val_accuracy: 0.7527\n",
            "Epoch 72/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4879 - accuracy: 0.8705\n",
            "Epoch 72: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 6s 613ms/step - loss: 0.4879 - accuracy: 0.8705 - val_loss: 0.9820 - val_accuracy: 0.7849\n",
            "Epoch 73/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4213 - accuracy: 0.8993\n",
            "Epoch 73: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 607ms/step - loss: 0.4213 - accuracy: 0.8993 - val_loss: 1.0277 - val_accuracy: 0.7634\n",
            "Epoch 74/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4433 - accuracy: 0.8921\n",
            "Epoch 74: val_loss did not improve from 0.84966\n",
            "9/9 [==============================] - 5s 605ms/step - loss: 0.4433 - accuracy: 0.8921 - val_loss: 0.9181 - val_accuracy: 0.8065\n",
            "Epoch 75/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.4076 - accuracy: 0.9029\n",
            "Epoch 75: val_loss improved from 0.84966 to 0.84307, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 609ms/step - loss: 0.4076 - accuracy: 0.9029 - val_loss: 0.8431 - val_accuracy: 0.8172\n",
            "Epoch 76/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3913 - accuracy: 0.9029\n",
            "Epoch 76: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 6s 614ms/step - loss: 0.3913 - accuracy: 0.9029 - val_loss: 0.8573 - val_accuracy: 0.8172\n",
            "Epoch 77/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3969 - accuracy: 0.9137\n",
            "Epoch 77: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 6s 619ms/step - loss: 0.3969 - accuracy: 0.9137 - val_loss: 0.9190 - val_accuracy: 0.8280\n",
            "Epoch 78/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3832 - accuracy: 0.9101\n",
            "Epoch 78: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 6s 614ms/step - loss: 0.3832 - accuracy: 0.9101 - val_loss: 0.9027 - val_accuracy: 0.8065\n",
            "Epoch 79/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3924 - accuracy: 0.9101\n",
            "Epoch 79: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 610ms/step - loss: 0.3924 - accuracy: 0.9101 - val_loss: 0.9315 - val_accuracy: 0.7957\n",
            "Epoch 80/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3612 - accuracy: 0.9173\n",
            "Epoch 80: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 605ms/step - loss: 0.3612 - accuracy: 0.9173 - val_loss: 0.8905 - val_accuracy: 0.8172\n",
            "Epoch 81/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3636 - accuracy: 0.9173\n",
            "Epoch 81: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 610ms/step - loss: 0.3636 - accuracy: 0.9173 - val_loss: 0.9276 - val_accuracy: 0.8172\n",
            "Epoch 82/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3682 - accuracy: 0.9137\n",
            "Epoch 82: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 601ms/step - loss: 0.3682 - accuracy: 0.9137 - val_loss: 0.9238 - val_accuracy: 0.8172\n",
            "Epoch 83/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3447 - accuracy: 0.9173\n",
            "Epoch 83: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 607ms/step - loss: 0.3447 - accuracy: 0.9173 - val_loss: 0.8537 - val_accuracy: 0.8387\n",
            "Epoch 84/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3654 - accuracy: 0.9101\n",
            "Epoch 84: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 603ms/step - loss: 0.3654 - accuracy: 0.9101 - val_loss: 0.8565 - val_accuracy: 0.8280\n",
            "Epoch 85/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3399 - accuracy: 0.9281\n",
            "Epoch 85: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 6s 616ms/step - loss: 0.3399 - accuracy: 0.9281 - val_loss: 0.8917 - val_accuracy: 0.8172\n",
            "Epoch 86/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3571 - accuracy: 0.9065\n",
            "Epoch 86: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 608ms/step - loss: 0.3571 - accuracy: 0.9065 - val_loss: 0.9902 - val_accuracy: 0.7742\n",
            "Epoch 87/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3673 - accuracy: 0.8993\n",
            "Epoch 87: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 614ms/step - loss: 0.3673 - accuracy: 0.8993 - val_loss: 0.9447 - val_accuracy: 0.7742\n",
            "Epoch 88/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3665 - accuracy: 0.9137\n",
            "Epoch 88: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 608ms/step - loss: 0.3665 - accuracy: 0.9137 - val_loss: 0.9682 - val_accuracy: 0.7742\n",
            "Epoch 89/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3518 - accuracy: 0.9137\n",
            "Epoch 89: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 6s 617ms/step - loss: 0.3518 - accuracy: 0.9137 - val_loss: 0.9618 - val_accuracy: 0.8065\n",
            "Epoch 90/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3310 - accuracy: 0.9209\n",
            "Epoch 90: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 605ms/step - loss: 0.3310 - accuracy: 0.9209 - val_loss: 0.9748 - val_accuracy: 0.7957\n",
            "Epoch 91/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3440 - accuracy: 0.9209\n",
            "Epoch 91: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 6s 615ms/step - loss: 0.3440 - accuracy: 0.9209 - val_loss: 0.9956 - val_accuracy: 0.8172\n",
            "Epoch 92/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3325 - accuracy: 0.9173\n",
            "Epoch 92: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.3325 - accuracy: 0.9173 - val_loss: 0.9335 - val_accuracy: 0.8172\n",
            "Epoch 93/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3346 - accuracy: 0.9173\n",
            "Epoch 93: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 608ms/step - loss: 0.3346 - accuracy: 0.9173 - val_loss: 0.8720 - val_accuracy: 0.8172\n",
            "Epoch 94/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3102 - accuracy: 0.9245\n",
            "Epoch 94: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.3102 - accuracy: 0.9245 - val_loss: 0.8834 - val_accuracy: 0.8280\n",
            "Epoch 95/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3160 - accuracy: 0.9281\n",
            "Epoch 95: val_loss did not improve from 0.84307\n",
            "9/9 [==============================] - 5s 608ms/step - loss: 0.3160 - accuracy: 0.9281 - val_loss: 0.8648 - val_accuracy: 0.8280\n",
            "Epoch 96/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3266 - accuracy: 0.9173\n",
            "Epoch 96: val_loss improved from 0.84307 to 0.83755, saving model to /tmp/video_classifier\n",
            "9/9 [==============================] - 5s 611ms/step - loss: 0.3266 - accuracy: 0.9173 - val_loss: 0.8376 - val_accuracy: 0.8495\n",
            "Epoch 97/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3201 - accuracy: 0.9137\n",
            "Epoch 97: val_loss did not improve from 0.83755\n",
            "9/9 [==============================] - 5s 606ms/step - loss: 0.3201 - accuracy: 0.9137 - val_loss: 0.8609 - val_accuracy: 0.8172\n",
            "Epoch 98/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3272 - accuracy: 0.9209\n",
            "Epoch 98: val_loss did not improve from 0.83755\n",
            "9/9 [==============================] - 5s 610ms/step - loss: 0.3272 - accuracy: 0.9209 - val_loss: 0.9289 - val_accuracy: 0.7957\n",
            "Epoch 99/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3066 - accuracy: 0.9245\n",
            "Epoch 99: val_loss did not improve from 0.83755\n",
            "9/9 [==============================] - 5s 605ms/step - loss: 0.3066 - accuracy: 0.9245 - val_loss: 0.9183 - val_accuracy: 0.8172\n",
            "Epoch 100/100\n",
            "9/9 [==============================] - ETA: 0s - loss: 0.3122 - accuracy: 0.9173\n",
            "Epoch 100: val_loss did not improve from 0.83755\n",
            "9/9 [==============================] - 5s 607ms/step - loss: 0.3122 - accuracy: 0.9173 - val_loss: 0.8887 - val_accuracy: 0.8280\n"
          ]
        }
      ],
      "source": [
        "# Utility for running experiments.\n",
        "\n",
        "filepath = \"/tmp/video_classifier\"\n",
        "checkpoint = keras.callbacks.ModelCheckpoint(\n",
        "    filepath, save_weights_only=True, save_best_only=True, verbose=1\n",
        "    )\n",
        "seq_model = get_sequence_model()\n",
        "\n",
        "history = seq_model.fit(\n",
        "    [train_data[0], train_data[1]],\n",
        "    train_labels,\n",
        "    validation_split=0.25,\n",
        "    epochs=100,\n",
        "    callbacks=[checkpoint],\n",
        "    )\n",
        "#_, sequence_model = run_experiment()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_FINXhC7Wbn",
        "outputId": "d8e6c624-f55a-4388-9568-11573331fdad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7faef153f450>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "seq_model.load_weights(filepath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ULUrIi3Eo-K5",
        "outputId": "a44c9f07-79af-4567-8cba-dcecef549204"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy:  91.72661900520325\n",
            "Training Loss:  0.3122444748878479\n",
            "-----------------------------------------\n",
            "Validation accuracy:  82.79569745063782\n",
            "Validation Loss:  0.8887086510658264\n",
            "-----------------------------------------\n",
            "\n",
            "3/3 [==============================] - 0s 55ms/step - loss: 0.8007 - accuracy: 0.7742\n",
            "Test accuracy: 77.42%\n"
          ]
        }
      ],
      "source": [
        "print(\"Training accuracy: \", history.history['accuracy'][-1]*100)\n",
        "print(\"Training Loss: \", history.history['loss'][-1])\n",
        "print(\"-----------------------------------------\")\n",
        "print(\"Validation accuracy: \", history.history['val_accuracy'][-1]*100)\n",
        "print(\"Validation Loss: \", history.history['val_loss'][-1])\n",
        "print(\"-----------------------------------------\\n\")\n",
        "_, accuracy = seq_model.evaluate([test_data[0], test_data[1]], test_labels);\n",
        "print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "N3Zb69Pym0B2",
        "outputId": "01c542a0-1892-4166-9a05-15aa029b5cd8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.figure(1)\n",
        "\n",
        "# summarize history for accuracy\n",
        "\n",
        "plt.subplot(211)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='lower right')\n",
        "\n",
        "# summarize history for loss\n",
        "\n",
        "plt.subplot(212)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training', 'Validation'], loc='upper right')\n",
        "\n",
        "plt.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXpoJcR-adzN"
      },
      "source": [
        "save the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Es6jzD1xx7vh"
      },
      "outputs": [],
      "source": [
        "sequence_model = seq_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OvC7vcZECcQE",
        "outputId": "27ef3b25-ae88-4841-8726-236a60bf7260"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_4_layer_call_fn, gru_cell_4_layer_call_and_return_conditional_losses, gru_cell_5_layer_call_fn, gru_cell_5_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/saved_model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/saved_model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7faef22c0e50> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7fb060061f90> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ],
      "source": [
        "sequence_model.save(folder_root+'saved_model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XS4B-_igbOgg"
      },
      "source": [
        "load the saved model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiY7v2Z9XEJB"
      },
      "outputs": [],
      "source": [
        "sequence_model = keras.models.load_model(folder_root+'saved_model')\n",
        "\n",
        "#sequence_model = keras.models.load_model('inceptionv3.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ol6h1VObbTx4"
      },
      "source": [
        "## Inference code for testing on single video\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJP_3J2VwXvA"
      },
      "outputs": [],
      "source": [
        "import imageio\n",
        "from tensorflow_docs.vis import embed\n",
        "\n",
        "def prepare_single_video(path):\n",
        "       # For each video.\n",
        "       frames = load_video(path)\n",
        "       frames = frames[None, ...]\n",
        "       \n",
        "       gc.collect()\n",
        "       # Initialize placeholders to store the masks and features of the current video.\n",
        "       \n",
        "       temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
        "       temp_frame_features = np.zeros(shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\")\n",
        "       \n",
        "       # Extract features from the frames of the current video.\n",
        "       for i, batch in enumerate(frames):\n",
        "           try:\n",
        "             video_length = batch.shape[1]\n",
        "             length = min(MAX_SEQ_LENGTH, video_length)\n",
        "             for j in range(length):\n",
        "                temp_frame_features[i, j, :] = feature_extractor.predict(batch[None, j, :])\n",
        "             temp_frame_mask[i, :length] = 1  # 1 = not masked, 0 = masked\n",
        "            #  frame_features = temp_frame_features.squeeze()\n",
        "            #  frame_masks = temp_frame_mask.squeeze()\n",
        "           except:\n",
        "             #print(i, j, length)\n",
        "             pass\n",
        "        \n",
        "       return temp_frame_features, temp_frame_mask\n",
        "\n",
        "def sequence_prediction(path):\n",
        "    class_vocab = label_processor.get_vocabulary()\n",
        "\n",
        "    frames = load_video(path)\n",
        "    frame_features, frame_mask = prepare_single_video(path)\n",
        "    probabilities = sequence_model.predict([frame_features, frame_mask])[0]\n",
        "    \n",
        "    v = ['call_the_ambulance','i_am_a_student','i_can_not_speak', 'how_are_you', \"i_don't_understand\", 'extra_class'] # 5 a, 5 b\n",
        "\n",
        "    for i in np.argsort(probabilities)[::-1]: #Add [::-1][:no] for starting fix number samples\n",
        "        print(f\"  {str(v[class_vocab[i].astype(int)])} :{class_vocab[i]}: {probabilities[i] * 100:5.2f}%\")\n",
        "\n",
        "    \n",
        "    return frames\n",
        "\n",
        "\n",
        "def to_gif(images):\n",
        "    converted_images = images.astype(np.uint8)\n",
        "    imageio.mimsave(\"animation.gif\", converted_images, fps=25)\n",
        "    return embed.embed_file(\"animation.gif\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDnkUCzun_5f",
        "outputId": "1a1e7ecd-3778-4fc3-ca62-21dc631e3b32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test video path: /content/gdrive/MyDrive/sign_videos_sample_a_6/call_the_ambulance/call_the_ambulance-1.mp4\n"
          ]
        }
      ],
      "source": [
        "test_video = np.random.choice(test_df[\"path\"].values.tolist())\n",
        "#test_video = '/content/gdrive/MyDrive/sign_videos_sample_a_6/i_can_not_speak/i_can_not_speak-16.mp4'\n",
        "print(f\"Test video path: {test_video}\")\n",
        "test_frames = sequence_prediction(test_video)\n",
        "#to_gif(test_frames[:MAX_SEQ_LENGTH])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "FYP_Inception_V3_Sign_Video_Recognition.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}